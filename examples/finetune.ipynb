{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarnyQXQ/new_repo/blob/main/examples/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cvz-uzzsgy7p",
        "outputId": "e0b0c61b-d58c-4b58-af75-dbbecf76bcf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM-Tuning'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 163 (delta 81), reused 65 (delta 65), pack-reused 40\u001b[K\n",
            "Receiving objects: 100% (163/163), 9.33 MiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "/content/ChatGLM-Tuning\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 15))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-kk04wgv8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-kk04wgv8\n",
            "  Resolved https://github.com/huggingface/peft.git to commit e06d94ddeb6c70913593740618df76908b918d66\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes==0.37.1 (from -r requirements.txt (line 2))\n",
            "  Downloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.17.1 (from -r requirements.txt (line 3))\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<3.20.1,>=3.19.5 (from -r requirements.txt (line 6))\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.27.1 (from -r requirements.txt (line 7))\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting icetk (from -r requirements.txt (line 8))\n",
            "  Downloading icetk-0.0.7-py3-none-any.whl (16 kB)\n",
            "Collecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.0.1+cu118)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.12.3)\n",
            "Collecting datasets==2.10.1 (from -r requirements.txt (line 14))\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.1->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r requirements.txt (line 7))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1->-r requirements.txt (line 14))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (1.5.3)\n",
            "Collecting xxhash (from datasets==2.10.1->-r requirements.txt (line 14))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.10.1->-r requirements.txt (line 14))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (3.8.4)\n",
            "Collecting responses<0.19 (from datasets==2.10.1->-r requirements.txt (line 14))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from icetk->-r requirements.txt (line 8)) (0.15.2+cu118)\n",
            "Collecting sentencepiece (from icetk->-r requirements.txt (line 8))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of icetk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting icetk (from -r requirements.txt (line 8))\n",
            "  Downloading icetk-0.0.6-py3-none-any.whl (15 kB)\n",
            "  Downloading icetk-0.0.5-py3-none-any.whl (15 kB)\n",
            "  Downloading icetk-0.0.4-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 10)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 10)) (16.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 11)) (0.40.0)\n",
            "Collecting safetensors (from peft==0.5.0.dev0->-r requirements.txt (line 15))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 11)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 11)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->icetk->-r requirements.txt (line 8)) (8.4.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 11)) (3.2.2)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.5.0.dev0-py3-none-any.whl size=72815 sha256=63e96053e6e99caf2777e15ad43852bd0bfa19c81cdb77d459e3efb9604715ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-40nzb_zp/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "Successfully built peft\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, cpm_kernels, bitsandbytes, xxhash, protobuf, dill, responses, multiprocess, huggingface-hub, transformers, datasets, accelerate, peft, icetk\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.13.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.17.1 bitsandbytes-0.37.1 cpm_kernels-1.0.11 datasets-2.10.1 dill-0.3.6 huggingface-hub-0.16.4 icetk-0.0.4 multiprocess-0.70.14 peft-0.5.0.dev0 protobuf-3.20.0 responses-0.18.0 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.27.1 xxhash-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/mymusise/ChatGLM-Tuning.git\n",
        "%cd  ChatGLM-Tuning\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CnUZ8kibgy7q",
        "outputId": "76937c7b-5b74-4b6e-e378-84cdf3338227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "formatting..: 100% 52002/52002 [00:00<00:00, 77260.09it/s]\n"
          ]
        }
      ],
      "source": [
        "!python cover_alpaca2jsonl.py \\\n",
        "    --data_path data/alpaca_data.json \\\n",
        "    --save_path data/alpaca_data.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b2okHe2mgy7r",
        "outputId": "04c2f45f-b824-45e6-ec39-479fae217f53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-b6b8095638fbee26/0.0.0...\n",
            "Generating train split: 0 examples [00:00, ? examples/s]\n",
            "Downloading (…)okenizer_config.json: 100% 441/441 [00:00<00:00, 1.81MB/s]\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "\n",
            "Downloading (…)enization_chatglm.py: 100% 17.0k/17.0k [00:00<00:00, 44.9MB/s]\n",
            "\n",
            "Downloading ice_text.model: 100% 2.71M/2.71M [00:00<00:00, 125MB/s]\n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 773/773 [00:00<00:00, 3.37MB/s]\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "\n",
            "Downloading (…)iguration_chatglm.py: 100% 4.28k/4.28k [00:00<00:00, 16.3MB/s]\n",
            "\n",
            "Generating train split: 1 examples [00:08,  8.05s/ examples]\n",
            "Generating train split: 54 examples [00:08,  9.36 examples/s]\n",
            "Generating train split: 122 examples [00:08, 25.63 examples/s]\n",
            "Generating train split: 191 examples [00:08, 47.81 examples/s]\n",
            "Generating train split: 259 examples [00:08, 76.57 examples/s]\n",
            "Generating train split: 323 examples [00:08, 110.83 examples/s]\n",
            "Generating train split: 415 examples [00:08, 168.68 examples/s]\n",
            "Generating train split: 500 examples [00:08, 224.32 examples/s]\n",
            "Generating train split: 563 examples [00:08, 271.61 examples/s]\n",
            "Generating train split: 631 examples [00:09, 329.76 examples/s]\n",
            "  1% 642/52002 [00:01<01:22, 621.34it/s]\u001b[A\n",
            "Generating train split: 719 examples [00:09, 388.51 examples/s]\n",
            "Generating train split: 791 examples [00:09, 446.34 examples/s]\n",
            "Generating train split: 884 examples [00:09, 492.01 examples/s]\n",
            "  2% 903/52002 [00:01<01:22, 620.45it/s]\u001b[A\n",
            "Generating train split: 972 examples [00:09, 516.79 examples/s]\n",
            "Generating train split: 1067 examples [00:10, 384.79 examples/s]\n",
            "Generating train split: 1128 examples [00:10, 419.18 examples/s]\n",
            "Generating train split: 1192 examples [00:10, 459.85 examples/s]\n",
            "Generating train split: 1249 examples [00:10, 479.43 examples/s]\n",
            "Generating train split: 1310 examples [00:10, 505.86 examples/s]\n",
            "Generating train split: 1381 examples [00:10, 552.61 examples/s]\n",
            "Generating train split: 1499 examples [00:10, 712.02 examples/s]\n",
            "Generating train split: 1600 examples [00:10, 790.09 examples/s]\n",
            "Generating train split: 1696 examples [00:10, 835.46 examples/s]\n",
            "Generating train split: 1786 examples [00:10, 851.19 examples/s]\n",
            "Generating train split: 1889 examples [00:11, 902.19 examples/s]\n",
            "Generating train split: 1997 examples [00:11, 952.43 examples/s]\n",
            "  4% 2023/52002 [00:03<00:54, 922.31it/s]\u001b[A\n",
            "Generating train split: 2143 examples [00:11, 957.13 examples/s]\n",
            "Generating train split: 2253 examples [00:11, 993.79 examples/s]\n",
            "Generating train split: 2367 examples [00:11, 1027.64 examples/s]\n",
            "Generating train split: 2471 examples [00:11, 1029.10 examples/s]\n",
            "Generating train split: 2584 examples [00:11, 1056.41 examples/s]\n",
            "Generating train split: 2724 examples [00:11, 1008.01 examples/s]\n",
            "Generating train split: 2867 examples [00:11, 983.74 examples/s] \n",
            "Generating train split: 2967 examples [00:12, 987.02 examples/s]\n",
            "  6% 2993/52002 [00:04<00:49, 989.19it/s]\u001b[A\n",
            "Generating train split: 3110 examples [00:12, 970.38 examples/s]\n",
            "Generating train split: 3219 examples [00:12, 995.33 examples/s]\n",
            "Generating train split: 3366 examples [00:12, 987.35 examples/s]\n",
            "Generating train split: 3468 examples [00:12, 993.62 examples/s]\n",
            "Generating train split: 3594 examples [00:12, 936.33 examples/s]\n",
            "Generating train split: 3695 examples [00:12, 952.29 examples/s]\n",
            "  7% 3709/52002 [00:04<00:50, 952.43it/s]\u001b[A\n",
            "Generating train split: 3835 examples [00:12, 940.75 examples/s]\n",
            "Generating train split: 3945 examples [00:13, 975.02 examples/s]\n",
            "Generating train split: 4069 examples [00:13, 922.18 examples/s]\n",
            "Generating train split: 4164 examples [00:13, 926.29 examples/s]\n",
            "Generating train split: 4278 examples [00:13, 979.76 examples/s]\n",
            "Generating train split: 4385 examples [00:13, 1000.39 examples/s]\n",
            "Generating train split: 4488 examples [00:13, 1007.99 examples/s]\n",
            "Generating train split: 4639 examples [00:13, 1006.44 examples/s]\n",
            "Generating train split: 4741 examples [00:13, 1008.32 examples/s]\n",
            "  9% 4751/52002 [00:05<00:46, 1010.57it/s]\u001b[A\n",
            "Generating train split: 4886 examples [00:14, 991.35 examples/s] \n",
            "Generating train split: 4994 examples [00:14, 1011.71 examples/s]\n",
            "Generating train split: 5130 examples [00:14, 971.11 examples/s] \n",
            "Generating train split: 5232 examples [00:14, 979.81 examples/s]\n",
            " 10% 5266/52002 [00:06<00:47, 978.18it/s]\u001b[A\n",
            "Generating train split: 5381 examples [00:14, 979.76 examples/s]\n",
            "Generating train split: 5490 examples [00:14, 1001.72 examples/s]\n",
            "Generating train split: 5603 examples [00:14, 1031.37 examples/s]\n",
            "Generating train split: 5711 examples [00:14, 1037.36 examples/s]\n",
            "Generating train split: 5816 examples [00:14, 1039.18 examples/s]\n",
            "Generating train split: 5953 examples [00:15, 986.85 examples/s] \n",
            "Generating train split: 6058 examples [00:15, 978.51 examples/s]\n",
            "Generating train split: 6210 examples [00:15, 986.33 examples/s]\n",
            " 12% 6213/52002 [00:07<00:45, 999.93it/s]\u001b[A\n",
            "Generating train split: 6357 examples [00:15, 980.05 examples/s]\n",
            "Generating train split: 6458 examples [00:15, 983.68 examples/s]\n",
            "Generating train split: 6562 examples [00:15, 994.66 examples/s]\n",
            "Generating train split: 6664 examples [00:15, 999.34 examples/s]\n",
            "Generating train split: 6809 examples [00:15, 979.28 examples/s]\n",
            "Generating train split: 6909 examples [00:16, 980.97 examples/s]\n",
            " 13% 6933/52002 [00:08<00:45, 988.06it/s]\u001b[A\n",
            "Generating train split: 7039 examples [00:16, 938.18 examples/s]\n",
            "Generating train split: 7145 examples [00:16, 964.69 examples/s]\n",
            "Generating train split: 7244 examples [00:16, 970.30 examples/s]\n",
            "Generating train split: 7344 examples [00:16, 978.01 examples/s]\n",
            "Generating train split: 7486 examples [00:16, 962.90 examples/s]\n",
            "Generating train split: 7591 examples [00:16, 981.70 examples/s]\n",
            "Generating train split: 7704 examples [00:16, 1017.38 examples/s]\n",
            "Generating train split: 7841 examples [00:17, 975.83 examples/s] \n",
            "Generating train split: 7943 examples [00:17, 984.89 examples/s]\n",
            " 15% 7961/52002 [00:09<00:44, 988.01it/s]\u001b[A\n",
            "Generating train split: 8080 examples [00:17, 958.73 examples/s]\n",
            "Generating train split: 8228 examples [00:17, 964.02 examples/s]\n",
            "Generating train split: 8327 examples [00:17, 968.02 examples/s]\n",
            " 16% 8356/52002 [00:09<00:45, 967.46it/s]\u001b[A\n",
            "Generating train split: 8474 examples [00:17, 964.86 examples/s]\n",
            "Generating train split: 8589 examples [00:17, 1007.84 examples/s]\n",
            "Generating train split: 8692 examples [00:17, 1010.33 examples/s]\n",
            "Generating train split: 8845 examples [00:18, 1012.75 examples/s]\n",
            "Generating train split: 8955 examples [00:18, 1033.16 examples/s]\n",
            " 17% 8988/52002 [00:10<00:41, 1045.42it/s]\u001b[A\n",
            "Generating train split: 9096 examples [00:18, 990.72 examples/s] \n",
            "Generating train split: 9247 examples [00:18, 994.19 examples/s]\n",
            "Generating train split: 9353 examples [00:18, 1004.34 examples/s]\n",
            "Generating train split: 9506 examples [00:18, 1006.22 examples/s]\n",
            " 18% 9511/52002 [00:10<00:42, 1008.97it/s]\u001b[A\n",
            "Generating train split: 9639 examples [00:18, 958.75 examples/s] \n",
            "Generating train split: 9778 examples [00:19, 944.84 examples/s]\n",
            " 19% 9804/52002 [00:11<00:46, 911.44it/s]\u001b[A\n",
            "Generating train split: 9904 examples [00:19, 910.55 examples/s]\n",
            "Generating train split: 10000 examples [00:19, 906.63 examples/s]\n",
            "Generating train split: 10100 examples [00:19, 925.55 examples/s]\n",
            "Generating train split: 10207 examples [00:19, 959.07 examples/s]\n",
            "Generating train split: 10313 examples [00:19, 984.69 examples/s]\n",
            "Generating train split: 10455 examples [00:19, 968.75 examples/s]\n",
            "Generating train split: 10568 examples [00:19, 1009.24 examples/s]\n",
            " 20% 10609/52002 [00:11<00:41, 1004.73it/s]\u001b[A\n",
            "Generating train split: 10716 examples [00:19, 997.07 examples/s] \n",
            "Generating train split: 10822 examples [00:20, 1011.39 examples/s]\n",
            "Generating train split: 10963 examples [00:20, 982.85 examples/s] \n",
            "Generating train split: 11103 examples [00:20, 962.88 examples/s]\n",
            " 21% 11119/52002 [00:12<00:42, 972.06it/s]\u001b[A\n",
            "Generating train split: 11226 examples [00:20, 913.06 examples/s]\n",
            "Generating train split: 11345 examples [00:20, 787.66 examples/s]\n",
            "Generating train split: 11447 examples [00:20, 752.35 examples/s]\n",
            "Generating train split: 11539 examples [00:21, 708.66 examples/s]\n",
            " 22% 11542/52002 [00:13<01:00, 672.28it/s]\u001b[A\n",
            "Generating train split: 11617 examples [00:21, 646.52 examples/s]\n",
            "Generating train split: 11684 examples [00:21, 649.06 examples/s]\n",
            "Generating train split: 11770 examples [00:21, 619.03 examples/s]\n",
            "Generating train split: 11837 examples [00:21, 628.72 examples/s]\n",
            "Generating train split: 11926 examples [00:21, 613.70 examples/s]\n",
            "Generating train split: 11991 examples [00:21, 619.59 examples/s]\n",
            " 23% 11993/52002 [00:13<01:05, 614.58it/s]\u001b[A\n",
            "Generating train split: 12075 examples [00:21, 592.87 examples/s]\n",
            "Generating train split: 12159 examples [00:22, 580.75 examples/s]\n",
            " 23% 12176/52002 [00:14<01:08, 581.12it/s]\u001b[A\n",
            "Generating train split: 12243 examples [00:22, 569.69 examples/s]\n",
            "Generating train split: 12312 examples [00:22, 595.09 examples/s]\n",
            "Generating train split: 12401 examples [00:22, 591.76 examples/s]\n",
            "Generating train split: 12463 examples [00:22, 596.44 examples/s]\n",
            "Generating train split: 12524 examples [00:22, 597.80 examples/s]\n",
            "Generating train split: 12606 examples [00:22, 571.26 examples/s]\n",
            "Generating train split: 12666 examples [00:22, 577.39 examples/s]\n",
            " 24% 12667/52002 [00:14<01:08, 574.86it/s]\u001b[A\n",
            "Generating train split: 12759 examples [00:23, 586.86 examples/s]\n",
            " 25% 12783/52002 [00:15<01:08, 573.14it/s]\u001b[A\n",
            "Generating train split: 12843 examples [00:23, 574.49 examples/s]\n",
            "Generating train split: 12906 examples [00:23, 584.80 examples/s]\n",
            "Generating train split: 12996 examples [00:23, 584.54 examples/s]\n",
            "Generating train split: 13072 examples [00:23, 555.32 examples/s]\n",
            "Generating train split: 13145 examples [00:23, 594.06 examples/s]\n",
            "Generating train split: 13208 examples [00:23, 598.87 examples/s]\n",
            "Generating train split: 13272 examples [00:24, 607.33 examples/s]\n",
            " 26% 13286/52002 [00:16<01:02, 615.30it/s]\u001b[A\n",
            "Generating train split: 13357 examples [00:24, 587.32 examples/s]\n",
            "Generating train split: 13417 examples [00:24, 588.58 examples/s]\n",
            "Generating train split: 13481 examples [00:24, 599.11 examples/s]\n",
            "Generating train split: 13550 examples [00:24, 623.07 examples/s]\n",
            "Generating train split: 13636 examples [00:24, 601.29 examples/s]\n",
            "Generating train split: 13722 examples [00:24, 590.10 examples/s]\n",
            "Generating train split: 13786 examples [00:24, 600.10 examples/s]\n",
            " 27% 13786/52002 [00:16<01:03, 603.34it/s]\u001b[A\n",
            "Generating train split: 13850 examples [00:24, 605.89 examples/s]\n",
            "Generating train split: 13915 examples [00:25, 615.51 examples/s]\n",
            "Generating train split: 14000 examples [00:25, 581.11 examples/s]\n",
            "Generating train split: 14084 examples [00:25, 642.64 examples/s]\n",
            "Generating train split: 14184 examples [00:25, 734.07 examples/s]\n",
            "Generating train split: 14284 examples [00:25, 805.65 examples/s]\n",
            "Generating train split: 14397 examples [00:25, 893.55 examples/s]\n",
            "Generating train split: 14499 examples [00:25, 925.71 examples/s]\n",
            "Generating train split: 14596 examples [00:25, 936.82 examples/s]\n",
            "Generating train split: 14692 examples [00:25, 940.01 examples/s]\n",
            "Generating train split: 14790 examples [00:26, 947.63 examples/s]\n",
            "Generating train split: 14898 examples [00:26, 981.83 examples/s]\n",
            " 29% 14937/52002 [00:18<00:38, 968.48it/s]\u001b[A\n",
            "Generating train split: 15037 examples [00:26, 956.33 examples/s]\n",
            "Generating train split: 15169 examples [00:26, 922.88 examples/s]\n",
            "Generating train split: 15275 examples [00:26, 956.50 examples/s]\n",
            "Generating train split: 15417 examples [00:26, 948.29 examples/s]\n",
            "Generating train split: 15518 examples [00:26, 959.53 examples/s]\n",
            "Generating train split: 15622 examples [00:26, 975.74 examples/s]\n",
            " 30% 15630/52002 [00:18<00:37, 980.88it/s]\u001b[A\n",
            "Generating train split: 15767 examples [00:27, 971.22 examples/s]\n",
            "Generating train split: 15867 examples [00:27, 974.42 examples/s]\n",
            "Generating train split: 15967 examples [00:27, 980.31 examples/s]\n",
            "Generating train split: 16105 examples [00:27, 955.73 examples/s]\n",
            "Generating train split: 16204 examples [00:27, 962.38 examples/s]\n",
            "Generating train split: 16308 examples [00:27, 982.70 examples/s]\n",
            " 31% 16331/52002 [00:19<00:36, 990.75it/s]\u001b[A\n",
            "Generating train split: 16451 examples [00:27, 970.43 examples/s]\n",
            "Generating train split: 16560 examples [00:27, 997.66 examples/s]\n",
            "Generating train split: 16717 examples [00:28, 1009.80 examples/s]\n",
            " 32% 16743/52002 [00:20<00:34, 1021.32it/s]\u001b[A\n",
            "Generating train split: 16870 examples [00:28, 1009.34 examples/s]\n",
            "Generating train split: 16974 examples [00:28, 1011.78 examples/s]\n",
            "Generating train split: 17098 examples [00:28, 945.66 examples/s] \n",
            "Generating train split: 17198 examples [00:28, 958.52 examples/s]\n",
            "Generating train split: 17301 examples [00:28, 976.46 examples/s]\n",
            "Generating train split: 17406 examples [00:28, 993.69 examples/s]\n",
            "Generating train split: 17514 examples [00:28, 1016.55 examples/s]\n",
            "Generating train split: 17666 examples [00:28, 1010.06 examples/s]\n",
            "Generating train split: 17769 examples [00:29, 1008.08 examples/s]\n",
            " 34% 17769/52002 [00:21<00:33, 1007.24it/s]\u001b[A\n",
            "Generating train split: 17881 examples [00:29, 1032.94 examples/s]\n",
            "Generating train split: 18011 examples [00:29, 966.93 examples/s] \n",
            "Generating train split: 18162 examples [00:29, 978.16 examples/s]\n",
            " 35% 18182/52002 [00:21<00:35, 965.14it/s]\u001b[A\n",
            "Generating train split: 18297 examples [00:29, 948.62 examples/s]\n",
            "Generating train split: 18440 examples [00:29, 947.18 examples/s]\n",
            " 36% 18470/52002 [00:21<00:36, 929.94it/s]\u001b[A\n",
            "Generating train split: 18581 examples [00:29, 938.83 examples/s]\n",
            "Generating train split: 18685 examples [00:30, 959.58 examples/s]\n",
            "Generating train split: 18786 examples [00:30, 968.77 examples/s]\n",
            "Generating train split: 18884 examples [00:30, 966.96 examples/s]\n",
            "Generating train split: 18987 examples [00:30, 982.36 examples/s]\n",
            "Generating train split: 19117 examples [00:30, 931.55 examples/s]\n",
            "Generating train split: 19223 examples [00:30, 962.25 examples/s]\n",
            "Generating train split: 19326 examples [00:30, 975.67 examples/s]\n",
            "Generating train split: 19465 examples [00:30, 957.03 examples/s]\n",
            " 37% 19486/52002 [00:22<00:33, 968.64it/s]\u001b[A\n",
            "Generating train split: 19591 examples [00:31, 913.84 examples/s]\n",
            "Generating train split: 19689 examples [00:31, 927.66 examples/s]\n",
            "Generating train split: 19796 examples [00:31, 962.90 examples/s]\n",
            "Generating train split: 19943 examples [00:31, 966.48 examples/s]\n",
            "Generating train split: 20086 examples [00:31, 960.00 examples/s]\n",
            " 39% 20089/52002 [00:23<00:33, 956.38it/s]\u001b[A\n",
            "Generating train split: 20208 examples [00:31, 906.49 examples/s]\n",
            "Generating train split: 20316 examples [00:31, 944.14 examples/s]\n",
            "Generating train split: 20422 examples [00:31, 968.05 examples/s]\n",
            "Generating train split: 20570 examples [00:32, 970.83 examples/s]\n",
            "Generating train split: 20676 examples [00:32, 991.29 examples/s]\n",
            "Generating train split: 20793 examples [00:32, 1033.81 examples/s]\n",
            "Generating train split: 20899 examples [00:32, 1037.73 examples/s]\n",
            " 40% 20916/52002 [00:24<00:30, 1030.41it/s]\u001b[A\n",
            "Generating train split: 21054 examples [00:32, 1003.11 examples/s]\n",
            "Generating train split: 21206 examples [00:32, 1003.77 examples/s]\n",
            "Generating train split: 21314 examples [00:32, 1021.06 examples/s]\n",
            " 41% 21332/52002 [00:24<00:29, 1029.11it/s]\u001b[A\n",
            "Generating train split: 21464 examples [00:32, 1011.83 examples/s]\n",
            "Generating train split: 21615 examples [00:33, 1007.10 examples/s]\n",
            "Generating train split: 21717 examples [00:33, 1007.44 examples/s]\n",
            " 42% 21743/52002 [00:25<00:30, 996.99it/s] \u001b[A\n",
            "Generating train split: 21871 examples [00:33, 1009.01 examples/s]\n",
            "Generating train split: 22000 examples [00:33, 958.26 examples/s] \n",
            "Generating train split: 22098 examples [00:33, 958.12 examples/s]\n",
            "Generating train split: 22199 examples [00:33, 969.36 examples/s]\n",
            "Generating train split: 22301 examples [00:33, 980.53 examples/s]\n",
            "Generating train split: 22400 examples [00:33, 979.83 examples/s]\n",
            "Generating train split: 22505 examples [00:33, 996.34 examples/s]\n",
            "Generating train split: 22650 examples [00:34, 982.04 examples/s]\n",
            "Generating train split: 22749 examples [00:34, 980.20 examples/s]\n",
            " 44% 22762/52002 [00:26<00:30, 967.10it/s]\u001b[A\n",
            "Generating train split: 22891 examples [00:34, 964.96 examples/s]\n",
            "Generating train split: 22996 examples [00:34, 983.26 examples/s]\n",
            "Generating train split: 23135 examples [00:34, 961.95 examples/s]\n",
            " 45% 23154/52002 [00:26<00:30, 954.57it/s]\u001b[A\n",
            "Generating train split: 23282 examples [00:34, 965.10 examples/s]\n",
            "Generating train split: 23395 examples [00:34, 1002.28 examples/s]\n",
            "Generating train split: 23506 examples [00:34, 1027.65 examples/s]\n",
            "Generating train split: 23660 examples [00:35, 1024.32 examples/s]\n",
            "Generating train split: 23771 examples [00:35, 1041.32 examples/s]\n",
            "Generating train split: 23895 examples [00:35, 860.10 examples/s] \n",
            " 46% 23896/52002 [00:27<00:33, 841.17it/s] \u001b[A\n",
            "Generating train split: 23990 examples [00:35, 786.11 examples/s]\n",
            "Generating train split: 24096 examples [00:35, 684.69 examples/s]\n",
            "Generating train split: 24179 examples [00:35, 646.54 examples/s]\n",
            " 47% 24208/52002 [00:27<00:44, 626.72it/s]\u001b[A\n",
            "Generating train split: 24274 examples [00:36, 641.69 examples/s]\n",
            "Generating train split: 24343 examples [00:36, 647.69 examples/s]\n",
            "Generating train split: 24438 examples [00:36, 639.40 examples/s]\n",
            "Generating train split: 24529 examples [00:36, 626.66 examples/s]\n",
            "Generating train split: 24596 examples [00:36, 633.77 examples/s]\n",
            " 47% 24605/52002 [00:28<00:44, 622.19it/s]\u001b[A\n",
            "Generating train split: 24692 examples [00:36, 631.47 examples/s]\n",
            "Generating train split: 24773 examples [00:36, 598.48 examples/s]\n",
            " 48% 24797/52002 [00:28<00:45, 595.70it/s]\u001b[A\n",
            "Generating train split: 24863 examples [00:37, 585.94 examples/s]\n",
            "Generating train split: 24953 examples [00:37, 586.89 examples/s]\n",
            "Generating train split: 25029 examples [00:37, 556.73 examples/s]\n",
            " 48% 25037/52002 [00:29<00:49, 541.81it/s]\u001b[A\n",
            "Generating train split: 25117 examples [00:37, 563.30 examples/s]\n",
            "Generating train split: 25176 examples [00:37, 567.34 examples/s]\n",
            "Generating train split: 25237 examples [00:37, 575.50 examples/s]\n",
            "Generating train split: 25317 examples [00:37, 557.42 examples/s]\n",
            " 49% 25334/52002 [00:29<00:47, 563.15it/s]\u001b[A\n",
            "Generating train split: 25403 examples [00:38, 558.33 examples/s]\n",
            "Generating train split: 25468 examples [00:38, 577.18 examples/s]\n",
            "Generating train split: 25530 examples [00:38, 583.90 examples/s]\n",
            "Generating train split: 25595 examples [00:38, 597.86 examples/s]\n",
            "Generating train split: 25686 examples [00:38, 596.26 examples/s]\n",
            " 49% 25705/52002 [00:30<00:44, 592.37it/s]\u001b[A\n",
            "Generating train split: 25767 examples [00:38, 573.88 examples/s]\n",
            "Generating train split: 25850 examples [00:38, 565.02 examples/s]\n",
            "Generating train split: 25926 examples [00:38, 540.76 examples/s]\n",
            "Generating train split: 25990 examples [00:39, 562.30 examples/s]\n",
            " 50% 25995/52002 [00:31<00:46, 556.26it/s]\u001b[A\n",
            "Generating train split: 26069 examples [00:39, 548.59 examples/s]\n",
            "Generating train split: 26134 examples [00:39, 570.40 examples/s]\n",
            "Generating train split: 26228 examples [00:39, 583.18 examples/s]\n",
            " 50% 26232/52002 [00:31<00:44, 577.95it/s]\u001b[A\n",
            "Generating train split: 26314 examples [00:39, 578.42 examples/s]\n",
            "Generating train split: 26377 examples [00:39, 587.05 examples/s]\n",
            "Generating train split: 26460 examples [00:39, 572.25 examples/s]\n",
            "Generating train split: 26531 examples [00:39, 601.43 examples/s]\n",
            "Generating train split: 26635 examples [00:40, 708.00 examples/s]\n",
            "Generating train split: 26719 examples [00:40, 742.58 examples/s]\n",
            "Generating train split: 26818 examples [00:40, 807.72 examples/s]\n",
            "Generating train split: 26923 examples [00:40, 868.83 examples/s]\n",
            " 52% 26935/52002 [00:32<00:28, 868.53it/s]\u001b[A\n",
            "Generating train split: 27061 examples [00:40, 885.75 examples/s]\n",
            "Generating train split: 27164 examples [00:40, 923.03 examples/s]\n",
            "Generating train split: 27265 examples [00:40, 944.34 examples/s]\n",
            "Generating train split: 27415 examples [00:40, 961.21 examples/s]\n",
            "Generating train split: 27521 examples [00:40, 983.87 examples/s]\n",
            " 53% 27542/52002 [00:32<00:24, 992.47it/s]\u001b[A\n",
            "Generating train split: 27665 examples [00:41, 974.28 examples/s]\n",
            "Generating train split: 27764 examples [00:41, 975.82 examples/s]\n",
            "Generating train split: 27911 examples [00:41, 975.56 examples/s]\n",
            " 54% 27951/52002 [00:33<00:24, 978.03it/s]\u001b[A\n",
            "Generating train split: 28057 examples [00:41, 953.97 examples/s]\n",
            "Generating train split: 28195 examples [00:41, 941.27 examples/s]\n",
            "Generating train split: 28291 examples [00:41, 944.34 examples/s]\n",
            "Generating train split: 28420 examples [00:41, 912.64 examples/s]\n",
            "Generating train split: 28521 examples [00:42, 931.48 examples/s]\n",
            " 55% 28526/52002 [00:34<00:25, 925.40it/s]\u001b[A\n",
            "Generating train split: 28654 examples [00:42, 914.34 examples/s]\n",
            "Generating train split: 28766 examples [00:42, 960.04 examples/s]\n",
            "Generating train split: 28875 examples [00:42, 989.94 examples/s]\n",
            "Generating train split: 28976 examples [00:42, 991.14 examples/s]\n",
            "Generating train split: 29120 examples [00:42, 976.42 examples/s]\n",
            "Generating train split: 29222 examples [00:42, 983.56 examples/s]\n",
            " 56% 29248/52002 [00:34<00:22, 995.20it/s]\u001b[A\n",
            "Generating train split: 29368 examples [00:42, 975.27 examples/s]\n",
            "Generating train split: 29473 examples [00:43, 992.31 examples/s]\n",
            "Generating train split: 29613 examples [00:43, 964.99 examples/s]\n",
            "Generating train split: 29717 examples [00:43, 981.68 examples/s]\n",
            "Generating train split: 29821 examples [00:43, 994.07 examples/s]\n",
            "Generating train split: 29963 examples [00:43, 974.14 examples/s]\n",
            " 58% 29969/52002 [00:35<00:22, 983.93it/s]\u001b[A\n",
            "Generating train split: 30096 examples [00:43, 941.38 examples/s]\n",
            "Generating train split: 30241 examples [00:43, 945.40 examples/s]\n",
            " 58% 30265/52002 [00:35<00:23, 941.46it/s]\u001b[A\n",
            "Generating train split: 30389 examples [00:43, 955.87 examples/s]\n",
            "Generating train split: 30522 examples [00:44, 928.82 examples/s]\n",
            " 59% 30559/52002 [00:36<00:23, 914.87it/s]\u001b[A\n",
            "Generating train split: 30665 examples [00:44, 932.47 examples/s]\n",
            "Generating train split: 30776 examples [00:44, 969.64 examples/s]\n",
            "Generating train split: 30921 examples [00:44, 967.33 examples/s]\n",
            "Generating train split: 31059 examples [00:44, 945.36 examples/s]\n",
            "Generating train split: 31157 examples [00:44, 951.59 examples/s]\n",
            " 60% 31165/52002 [00:36<00:21, 953.15it/s]\u001b[A\n",
            "Generating train split: 31302 examples [00:44, 950.69 examples/s]\n",
            "Generating train split: 31440 examples [00:45, 937.57 examples/s]\n",
            "Generating train split: 31548 examples [00:45, 968.75 examples/s]\n",
            "Generating train split: 31657 examples [00:45, 994.88 examples/s]\n",
            "Generating train split: 31766 examples [00:45, 1014.72 examples/s]\n",
            "Generating train split: 31879 examples [00:45, 1044.31 examples/s]\n",
            "Generating train split: 31986 examples [00:45, 1049.68 examples/s]\n",
            " 62% 31995/52002 [00:37<00:18, 1060.15it/s]\u001b[A\n",
            "Generating train split: 32124 examples [00:45, 998.00 examples/s] \n",
            "Generating train split: 32228 examples [00:45, 1008.07 examples/s]\n",
            "Generating train split: 32363 examples [00:45, 966.80 examples/s] \n",
            " 62% 32411/52002 [00:38<00:20, 956.12it/s]\u001b[A\n",
            "Generating train split: 32519 examples [00:46, 987.44 examples/s]\n",
            "Generating train split: 32625 examples [00:46, 1001.42 examples/s]\n",
            "Generating train split: 32759 examples [00:46, 961.85 examples/s] \n",
            "Generating train split: 32869 examples [00:46, 991.47 examples/s]\n",
            "Generating train split: 32978 examples [00:46, 1014.18 examples/s]\n",
            "Generating train split: 33117 examples [00:46, 979.67 examples/s] \n",
            "Generating train split: 33217 examples [00:46, 981.01 examples/s]\n",
            " 64% 33239/52002 [00:38<00:18, 994.77it/s]\u001b[A\n",
            "Generating train split: 33352 examples [00:47, 950.14 examples/s]\n",
            "Generating train split: 33454 examples [00:47, 964.93 examples/s]\n",
            "Generating train split: 33557 examples [00:47, 978.65 examples/s]\n",
            "Generating train split: 33664 examples [00:47, 1002.99 examples/s]\n",
            "Generating train split: 33801 examples [00:47, 967.24 examples/s] \n",
            "Generating train split: 33901 examples [00:47, 971.92 examples/s]\n",
            "Generating train split: 34035 examples [00:47, 940.07 examples/s]\n",
            "Generating train split: 34139 examples [00:47, 962.29 examples/s]\n",
            "Generating train split: 34241 examples [00:47, 975.68 examples/s]\n",
            " 66% 34253/52002 [00:39<00:18, 967.79it/s]\u001b[A\n",
            "Generating train split: 34388 examples [00:48, 968.65 examples/s]\n",
            "Generating train split: 34528 examples [00:48, 952.49 examples/s]\n",
            "Generating train split: 34632 examples [00:48, 970.25 examples/s]\n",
            "Generating train split: 34741 examples [00:48, 997.13 examples/s]\n",
            " 67% 34758/52002 [00:40<00:17, 1001.48it/s]\u001b[A\n",
            "Generating train split: 34890 examples [00:48, 994.22 examples/s]\n",
            "Generating train split: 34995 examples [00:48, 1006.01 examples/s]\n",
            "Generating train split: 35131 examples [00:48, 967.80 examples/s] \n",
            "Generating train split: 35239 examples [00:48, 991.43 examples/s]\n",
            " 68% 35263/52002 [00:40<00:17, 970.95it/s]\u001b[A\n",
            "Generating train split: 35377 examples [00:49, 959.63 examples/s]\n",
            "Generating train split: 35480 examples [00:49, 973.93 examples/s]\n",
            "Generating train split: 35587 examples [00:49, 995.15 examples/s]\n",
            "Generating train split: 35692 examples [00:49, 1006.11 examples/s]\n",
            "Generating train split: 35830 examples [00:49, 968.77 examples/s] \n",
            "Generating train split: 35929 examples [00:49, 971.47 examples/s]\n",
            "Generating train split: 36049 examples [00:49, 904.84 examples/s]\n",
            "Generating train split: 36145 examples [00:49, 913.76 examples/s]\n",
            "Generating train split: 36242 examples [00:50, 815.83 examples/s]\n",
            "Generating train split: 36336 examples [00:50, 550.88 examples/s]\n",
            " 70% 36336/52002 [00:42<00:29, 525.71it/s]\u001b[A\n",
            "Generating train split: 36421 examples [00:50, 418.86 examples/s]\n",
            "Generating train split: 36481 examples [00:50, 381.92 examples/s]\n",
            "Generating train split: 36538 examples [00:51, 268.70 examples/s]\n",
            " 70% 36541/52002 [00:43<01:03, 243.57it/s]\u001b[A\n",
            "Generating train split: 36579 examples [00:51, 262.58 examples/s]\n",
            "Generating train split: 36643 examples [00:51, 316.04 examples/s]\n",
            "Generating train split: 36699 examples [00:51, 357.17 examples/s]\n",
            "Generating train split: 36764 examples [00:51, 413.46 examples/s]\n",
            "Generating train split: 36826 examples [00:51, 457.46 examples/s]\n",
            "Generating train split: 36896 examples [00:52, 513.19 examples/s]\n",
            "Generating train split: 36960 examples [00:52, 541.63 examples/s]\n",
            "Generating train split: 37035 examples [00:52, 524.89 examples/s]\n",
            "Generating train split: 37099 examples [00:52, 551.79 examples/s]\n",
            "Generating train split: 37161 examples [00:52, 567.84 examples/s]\n",
            "Generating train split: 37226 examples [00:52, 587.75 examples/s]\n",
            "Generating train split: 37287 examples [00:52, 592.12 examples/s]\n",
            "Generating train split: 37373 examples [00:52, 583.47 examples/s]\n",
            " 72% 37381/52002 [00:44<00:25, 580.36it/s]\u001b[A\n",
            "Generating train split: 37460 examples [00:52, 578.65 examples/s]\n",
            "Generating train split: 37520 examples [00:53, 580.99 examples/s]\n",
            "Generating train split: 37586 examples [00:53, 531.15 examples/s]\n",
            "Generating train split: 37651 examples [00:53, 557.57 examples/s]\n",
            "Generating train split: 37713 examples [00:53, 573.59 examples/s]\n",
            "Generating train split: 37773 examples [00:53, 576.55 examples/s]\n",
            "Generating train split: 37837 examples [00:53, 591.23 examples/s]\n",
            "Generating train split: 37916 examples [00:53, 556.79 examples/s]\n",
            "Generating train split: 37974 examples [00:53, 559.20 examples/s]\n",
            " 73% 37985/52002 [00:45<00:24, 565.54it/s]\u001b[A\n",
            "Generating train split: 38058 examples [00:54, 541.75 examples/s]\n",
            "Generating train split: 38122 examples [00:54, 562.09 examples/s]\n",
            "Generating train split: 38191 examples [00:54, 593.67 examples/s]\n",
            "Generating train split: 38276 examples [00:54, 581.46 examples/s]\n",
            " 74% 38293/52002 [00:46<00:23, 595.71it/s]\u001b[A\n",
            "Generating train split: 38363 examples [00:54, 580.47 examples/s]\n",
            "Generating train split: 38456 examples [00:54, 589.85 examples/s]\n",
            "Generating train split: 38520 examples [00:54, 600.39 examples/s]\n",
            "Generating train split: 38581 examples [00:54, 602.56 examples/s]\n",
            "Generating train split: 38677 examples [00:55, 691.95 examples/s]\n",
            "Generating train split: 38775 examples [00:55, 767.32 examples/s]\n",
            "Generating train split: 38868 examples [00:55, 805.47 examples/s]\n",
            "Generating train split: 38966 examples [00:55, 853.75 examples/s]\n",
            "Generating train split: 39053 examples [00:55, 853.80 examples/s]\n",
            "Generating train split: 39153 examples [00:55, 895.98 examples/s]\n",
            "Generating train split: 39252 examples [00:55, 921.76 examples/s]\n",
            "Generating train split: 39363 examples [00:55, 975.22 examples/s]\n",
            "Generating train split: 39466 examples [00:55, 987.61 examples/s]\n",
            "Generating train split: 39570 examples [00:55, 1001.27 examples/s]\n",
            "Generating train split: 39673 examples [00:56, 1006.66 examples/s]\n",
            " 76% 39715/52002 [00:48<00:12, 994.30it/s] \u001b[A\n",
            "Generating train split: 39819 examples [00:56, 980.53 examples/s] \n",
            "Generating train split: 39968 examples [00:56, 977.80 examples/s]\n",
            "Generating train split: 40110 examples [00:56, 963.70 examples/s]\n",
            "Generating train split: 40209 examples [00:56, 967.04 examples/s]\n",
            "Generating train split: 40314 examples [00:56, 986.01 examples/s]\n",
            " 78% 40317/52002 [00:48<00:11, 985.38it/s]\u001b[A\n",
            "Generating train split: 40454 examples [00:56, 962.60 examples/s]\n",
            "Generating train split: 40567 examples [00:56, 1000.40 examples/s]\n",
            "Generating train split: 40716 examples [00:57, 993.95 examples/s] \n",
            "Generating train split: 40817 examples [00:57, 996.15 examples/s]\n",
            "Generating train split: 40918 examples [00:57, 997.26 examples/s]\n",
            " 79% 40931/52002 [00:49<00:10, 1009.17it/s]\u001b[A\n",
            "Generating train split: 41064 examples [00:57, 985.82 examples/s]\n",
            "Generating train split: 41211 examples [00:57, 980.76 examples/s]\n",
            "Generating train split: 41315 examples [00:57, 992.93 examples/s]\n",
            " 79% 41341/52002 [00:49<00:10, 1003.68it/s]\u001b[A\n",
            "Generating train split: 41463 examples [00:57, 988.28 examples/s]\n",
            "Generating train split: 41568 examples [00:57, 1001.84 examples/s]\n",
            "Generating train split: 41712 examples [00:58, 981.19 examples/s] \n",
            "Generating train split: 41841 examples [00:58, 937.14 examples/s]\n",
            "Generating train split: 41945 examples [00:58, 959.87 examples/s]\n",
            " 81% 41946/52002 [00:50<00:10, 957.35it/s]\u001b[A\n",
            "Generating train split: 42081 examples [00:58, 939.31 examples/s]\n",
            "Generating train split: 42177 examples [00:58, 943.50 examples/s]\n",
            "Generating train split: 42279 examples [00:58, 960.03 examples/s]\n",
            "Generating train split: 42430 examples [00:58, 970.40 examples/s]\n",
            "Generating train split: 42536 examples [00:58, 990.69 examples/s]\n",
            "Generating train split: 42651 examples [00:59, 1028.44 examples/s]\n",
            " 82% 42661/52002 [00:51<00:09, 1028.11it/s]\u001b[A\n",
            "Generating train split: 42784 examples [00:59, 970.83 examples/s] \n",
            "Generating train split: 42928 examples [00:59, 961.87 examples/s]\n",
            "Generating train split: 43063 examples [00:59, 938.16 examples/s]\n",
            "Generating train split: 43175 examples [00:59, 979.43 examples/s]\n",
            " 83% 43177/52002 [00:51<00:08, 982.11it/s]\u001b[A\n",
            "Generating train split: 43312 examples [00:59, 952.98 examples/s]\n",
            "Generating train split: 43440 examples [00:59, 916.48 examples/s]\n",
            " 84% 43467/52002 [00:52<00:11, 761.81it/s]\u001b[A\n",
            "Generating train split: 43561 examples [01:00, 484.84 examples/s]\n",
            "Generating train split: 43645 examples [01:00, 494.31 examples/s]\n",
            "Generating train split: 43745 examples [01:00, 573.07 examples/s]\n",
            "Generating train split: 43853 examples [01:00, 664.74 examples/s]\n",
            "Generating train split: 43961 examples [01:00, 747.78 examples/s]\n",
            "Generating train split: 44069 examples [01:01, 718.07 examples/s]\n",
            "Generating train split: 44162 examples [01:01, 427.82 examples/s]\n",
            " 85% 44173/52002 [00:53<00:18, 413.01it/s]\u001b[A\n",
            "Generating train split: 44272 examples [01:01, 452.85 examples/s]\n",
            "Generating train split: 44368 examples [01:01, 529.73 examples/s]\n",
            "Generating train split: 44468 examples [01:01, 614.62 examples/s]\n",
            "Generating train split: 44576 examples [01:02, 708.60 examples/s]\n",
            "Generating train split: 44666 examples [01:02, 749.60 examples/s]\n",
            "Generating train split: 44821 examples [01:02, 841.66 examples/s]\n",
            "Generating train split: 44914 examples [01:02, 861.03 examples/s]\n",
            " 86% 44919/52002 [00:54<00:07, 886.28it/s]\u001b[A\n",
            "Generating train split: 45054 examples [01:02, 881.59 examples/s]\n",
            "Generating train split: 45149 examples [01:02, 894.27 examples/s]\n",
            "Generating train split: 45258 examples [01:02, 940.29 examples/s]\n",
            "Generating train split: 45401 examples [01:02, 942.92 examples/s]\n",
            " 87% 45407/52002 [00:54<00:06, 957.83it/s]\u001b[A\n",
            "Generating train split: 45551 examples [01:03, 960.05 examples/s]\n",
            "Generating train split: 45665 examples [01:03, 1002.84 examples/s]\n",
            "Generating train split: 45824 examples [01:03, 1019.79 examples/s]\n",
            " 88% 45834/52002 [00:55<00:05, 1045.93it/s]\u001b[A\n",
            "Generating train split: 45961 examples [01:03, 981.75 examples/s] \n",
            "Generating train split: 46093 examples [01:03, 924.92 examples/s]\n",
            "Generating train split: 46198 examples [01:03, 950.30 examples/s]\n",
            "Generating train split: 46304 examples [01:03, 971.04 examples/s]\n",
            "Generating train split: 46406 examples [01:03, 980.19 examples/s]\n",
            "Generating train split: 46512 examples [01:04, 999.88 examples/s]\n",
            "Generating train split: 46651 examples [01:04, 968.73 examples/s]\n",
            " 90% 46658/52002 [00:56<00:05, 960.48it/s] \u001b[A\n",
            "Generating train split: 46801 examples [01:04, 976.46 examples/s]\n",
            "Generating train split: 46942 examples [01:04, 960.95 examples/s]\n",
            " 90% 46959/52002 [00:56<00:05, 970.63it/s]\u001b[A\n",
            "Generating train split: 47091 examples [01:04, 967.67 examples/s]\n",
            "Generating train split: 47230 examples [01:04, 951.01 examples/s]\n",
            "Generating train split: 47332 examples [01:04, 964.97 examples/s]\n",
            " 91% 47348/52002 [00:56<00:04, 948.33it/s]\u001b[A\n",
            "Generating train split: 47457 examples [01:05, 823.00 examples/s]\n",
            "Generating train split: 47549 examples [01:05, 757.74 examples/s]\n",
            "Generating train split: 47644 examples [01:05, 717.62 examples/s]\n",
            "Generating train split: 47725 examples [01:05, 660.74 examples/s]\n",
            "Generating train split: 47793 examples [01:05, 662.52 examples/s]\n",
            "Generating train split: 47871 examples [01:05, 609.43 examples/s]\n",
            " 92% 47877/52002 [00:57<00:07, 588.09it/s]\u001b[A\n",
            "Generating train split: 47965 examples [01:06, 612.21 examples/s]\n",
            "Generating train split: 48051 examples [01:06, 593.59 examples/s]\n",
            "Generating train split: 48116 examples [01:06, 603.57 examples/s]\n",
            " 93% 48130/52002 [00:58<00:06, 605.05it/s]\u001b[A\n",
            "Generating train split: 48203 examples [01:06, 591.97 examples/s]\n",
            "Generating train split: 48268 examples [01:06, 602.72 examples/s]\n",
            "Generating train split: 48346 examples [01:06, 572.07 examples/s]\n",
            "Generating train split: 48405 examples [01:06, 573.88 examples/s]\n",
            "Generating train split: 48465 examples [01:06, 577.91 examples/s]\n",
            "Generating train split: 48553 examples [01:07, 577.82 examples/s]\n",
            " 93% 48559/52002 [00:58<00:05, 577.40it/s]\u001b[A\n",
            "Generating train split: 48645 examples [01:07, 586.82 examples/s]\n",
            "Generating train split: 48729 examples [01:07, 576.95 examples/s]\n",
            "Generating train split: 48791 examples [01:07, 586.37 examples/s]\n",
            "Generating train split: 48855 examples [01:07, 588.33 examples/s]\n",
            " 94% 48864/52002 [00:59<00:05, 588.31it/s]\u001b[A\n",
            "Generating train split: 48944 examples [01:07, 587.06 examples/s]\n",
            "Generating train split: 49027 examples [01:07, 558.19 examples/s]\n",
            "Generating train split: 49085 examples [01:07, 561.48 examples/s]\n",
            "Generating train split: 49144 examples [01:08, 567.15 examples/s]\n",
            "Generating train split: 49210 examples [01:08, 586.80 examples/s]\n",
            "Generating train split: 49274 examples [01:08, 600.12 examples/s]\n",
            "Generating train split: 49338 examples [01:08, 609.34 examples/s]\n",
            "Generating train split: 49400 examples [01:08, 607.99 examples/s]\n",
            "Generating train split: 49464 examples [01:08, 614.48 examples/s]\n",
            "Generating train split: 49537 examples [01:08, 562.64 examples/s]\n",
            " 95% 49541/52002 [01:00<00:04, 565.19it/s]\u001b[A\n",
            "Generating train split: 49621 examples [01:08, 557.43 examples/s]\n",
            "Generating train split: 49682 examples [01:08, 568.23 examples/s]\n",
            "Generating train split: 49746 examples [01:09, 585.15 examples/s]\n",
            "Generating train split: 49809 examples [01:09, 593.57 examples/s]\n",
            "Generating train split: 49901 examples [01:09, 596.13 examples/s]\n",
            " 96% 49910/52002 [01:01<00:03, 590.17it/s]\u001b[A\n",
            "Generating train split: 49988 examples [01:09, 585.32 examples/s]\n",
            "Generating train split: 50061 examples [01:09, 549.29 examples/s]\n",
            "Generating train split: 50153 examples [01:09, 636.48 examples/s]\n",
            "Generating train split: 50255 examples [01:09, 732.30 examples/s]\n",
            "Generating train split: 50352 examples [01:09, 794.21 examples/s]\n",
            "Generating train split: 50447 examples [01:10, 835.10 examples/s]\n",
            "Generating train split: 50551 examples [01:10, 890.42 examples/s]\n",
            "Generating train split: 50645 examples [01:10, 900.38 examples/s]\n",
            "Generating train split: 50738 examples [01:10, 906.36 examples/s]\n",
            "Generating train split: 50853 examples [01:10, 972.54 examples/s]\n",
            "Generating train split: 50957 examples [01:10, 991.08 examples/s]\n",
            "Generating train split: 51058 examples [01:10, 970.11 examples/s]\n",
            "Generating train split: 51199 examples [01:10, 951.26 examples/s]\n",
            "Generating train split: 51304 examples [01:10, 974.33 examples/s]\n",
            "Generating train split: 51404 examples [01:11, 980.68 examples/s]\n",
            " 99% 51412/52002 [01:02<00:00, 987.48it/s]\u001b[A\n",
            "Generating train split: 51548 examples [01:11, 966.45 examples/s]\n",
            "Generating train split: 51650 examples [01:11, 976.94 examples/s]\n",
            "Generating train split: 51758 examples [01:11, 1000.01 examples/s]\n",
            "Generating train split: 51905 examples [01:11, 987.99 examples/s] \n",
            "100% 52002/52002 [01:03<00:00, 817.82it/s]\n",
            "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-b6b8095638fbee26/0.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "!python tokenize_dataset_rows.py \\\n",
        "    --jsonl_path data/alpaca_data.jsonl \\\n",
        "    --save_path data/alpaca \\\n",
        "    --max_seq_length 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX8TKDXlgy7r"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZrQL06Egy7r",
        "outputId": "c86a2737-d080-4f7b-96fc-7f7727964971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
            "CUDA SETUP: Detected CUDA version 116\n",
            "CUDA SETUP: Loading binary /home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.04it/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "\n",
        "\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
        "model.supports_gradient_checkpointing = True\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YusJpTAUgy7s",
        "outputId": "7e8f4b19-024e-4140-b961-0ade0964dbc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae-llkUGgy7s"
      },
      "source": [
        "## Test before finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXdFvVLAgy7t",
        "outputId": "bf367249-e456-4793-c906-d3ae7c1c0e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Give three tips for staying healthy.\n",
            "Answer: I'm sorry, but I'm not sure what you're asking for. Could you please provide more context or clarify your question?\n",
            "### 1.Answer:\n",
            " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule. \n",
            "\n",
            "\n",
            "Instruction: What are the three primary colors?\n",
            "Answer: The three primary colors in painting are red, blue, and green. These colors are often used in combination to create more complex and vibrant colors.\n",
            "### 2.Answer:\n",
            " The three primary colors are red, blue, and yellow. \n",
            "\n",
            "\n",
            "Instruction: Describe the structure of an atom.\n",
            "Answer: The原子是构成物质的基本单位,由带正电荷的质子和不带电荷的电子组成。原子核由带正电荷的质子和不带电荷的中子组成,它们之间通过核力相互吸引。\n",
            "\n",
            "原子的化学性质取决于其组成和结构,以及化学反应中所涉及的因素。例如,氧原子是化学反应中最常见的原子之一,因为它具有两个电子,可以与许多其他原子形成化合物。\n",
            "\n",
            "物质是由原子和分子组成的,而原子是物质的基本单位。了解原子的结构、性质以及化学反应,可以帮助我们更好地理解物质世界,并更好地利用这些物质来制造产品、治疗疾病。\n",
            "### 3.Answer:\n",
            " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
            "\n",
            "\n",
            "Instruction: How can we reduce air pollution?\n",
            "Answer: I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?\n",
            "### 4.Answer:\n",
            " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
            "\n",
            "\n",
            "Instruction: Describe a time when you had to make a difficult decision.\n",
            "Answer: I'm sorry, but I'm not sure what you're asking for. Could you please provide more context or clarify your question?\n",
            "### 5.Answer:\n",
            " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from cover_alpaca2jsonl import format_example\n",
        "import json\n",
        "\n",
        "\n",
        "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, item in enumerate(instructions[:5]):\n",
        "        feature = format_example(item)\n",
        "        input_text = feature[\"context\"]\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            temperature=0\n",
        "        )\n",
        "        answer = tokenizer.decode(out[0])\n",
        "        print(answer)\n",
        "        item['infer_answer'] = answer\n",
        "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNHmIXdkgy7t"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32, lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.is_parallelizable = True\n",
        "model.model_parallel = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHyQk57qgy7u"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "dataset_path = \"data/alpaca/\"\n",
        "\n",
        "dataset = datasets.load_from_disk(dataset_path)\n",
        "\n",
        "train_num = 500\n",
        "\n",
        "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0X949Bmgy7u"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, HfArgumentParser\n",
        "\n",
        "\n",
        "def data_collator(features: list) -> dict:\n",
        "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
        "    longest = max(len_ids)\n",
        "    input_ids = []\n",
        "    labels_list = []\n",
        "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
        "        ids = feature[\"input_ids\"]\n",
        "        seq_len = feature[\"seq_len\"]\n",
        "        labels = (\n",
        "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
        "        )\n",
        "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
        "        _ids = torch.LongTensor(ids)\n",
        "        labels_list.append(torch.LongTensor(labels))\n",
        "        input_ids.append(_ids)\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    labels = torch.stack(labels_list)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "class ModifiedTrainer(Trainer):\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        return model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            labels=inputs[\"labels\"],\n",
        "        ).loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXtBwQMrgy7u",
        "outputId": "e3b81a8d-2b39-4513-a137-dc94f77155c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 07:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.787700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.978900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.534500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.611100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.620300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.493800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.489700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.440500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.468200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.942300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.184100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.970900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.265900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.715500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.326100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=1.4622032979329427, metrics={'train_runtime': 474.9934, 'train_samples_per_second': 3.158, 'train_steps_per_second': 3.158, 'total_flos': 3781851053211648.0, 'train_loss': 1.4622032979329427, 'epoch': 3.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"output\",\n",
        "    fp16 =True,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    learning_rate = 1e-4,\n",
        "    max_steps=1500,\n",
        "    logging_steps=50,\n",
        "    remove_unused_columns=False,\n",
        "    seed=0,\n",
        "    data_seed=0,\n",
        "    group_by_length=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = ModifiedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=mini_train_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ped-ji-tgy7u"
      },
      "source": [
        "## Test After finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfCdMg9dgy7u",
        "outputId": "fd5480ff-939e-4064-f831-d4f06acac4f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mymusise/pro/stable-diffusion-webui/venv/lib/python3.8/site-packages/transformers-4.27.0.dev0-py3.8.egg/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Give three tips for staying healthy.\n",
            "Answer: 1. Eat a balanced diet. \n",
            "2. Get regular exercise. \n",
            "3. Stay hydrated by drinking plenty of water.\n",
            "### 1.Answer:\n",
            " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
            "2. Exercise regularly to keep your body active and strong. \n",
            "3. Get enough sleep and maintain a consistent sleep schedule. \n",
            "\n",
            "\n",
            "Instruction: What are the three primary colors?\n",
            "Answer: The three primary colors are red, blue, and yellow.\n",
            "### 2.Answer:\n",
            " The three primary colors are red, blue, and yellow. \n",
            "\n",
            "\n",
            "Instruction: Describe the structure of an atom.\n",
            "Answer: An atom is a small particle of a chemical element, with a central electron surrounded by other electrons, which form a cloud around the central electron. The cloud of electrons is surrounded by a cloud of positive ions, which make up the原子's positive charge. The positive ions and negative ions are attracted to each other, and the atoms form a cloud of ions and electrons.\n",
            "### 3.Answer:\n",
            " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
            "\n",
            "\n",
            "Instruction: How can we reduce air pollution?\n",
            "Answer: There are several ways to reduce air pollution, including reducing energy consumption, improving transportation, reducing waste and reducing the use of harmful chemicals. Additionally, increasing public awareness and education, implementing policies, and increasing funding for research can also help reduce air pollution.\n",
            "### 4.Answer:\n",
            " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
            "\n",
            "\n",
            "Instruction: Describe a time when you had to make a difficult decision.\n",
            "Answer: I had to make a difficult decision when I was in my 20s. I had to choose between my career and my studies. I knew that my studies were more important, but I also knew that I wanted to be a teacher. I had to make a decision based on my values and my future plans. I decided to pursue my studies and became a teacher. It was a difficult decision, but I was determined to make the right choice.\n",
            "### 5.Answer:\n",
            " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from cover_alpaca2jsonl import format_example\n",
        "import json\n",
        "\n",
        "\n",
        "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, item in enumerate(instructions[:5]):\n",
        "        feature = format_example(item)\n",
        "        input_text = feature[\"context\"]\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            temperature=0\n",
        "        )\n",
        "        answer = tokenizer.decode(out[0])\n",
        "        print(answer)\n",
        "        item['infer_answer'] = answer\n",
        "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHyX2kQmgy7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def save_tunable_parameters(model, path):\n",
        "    saved_params = {\n",
        "        k: v.to(\"cpu\")\n",
        "        for k, v in model.named_parameters()\n",
        "        if v.requires_grad\n",
        "    }\n",
        "    torch.save(saved_params, path)\n",
        "\n",
        "\n",
        "save_tunable_parameters(model, os.path.join(\"output\", \"chatglm-lora.pt\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Feb  7 2022, 13:32:35) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}